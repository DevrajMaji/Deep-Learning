{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is TensorFlow Object Detection API (TFOD2) and what are its\n",
        "primary components?\n",
        "\n",
        "->TensorFlow Object Detection API (TFOD2) is an open-source framework developed by Google using TensorFlow 2 for building, training, evaluating, and deploying object detection models. It helps in identifying and locating objects in images or videos by drawing bounding boxes and assigning class labels.\n",
        "\n",
        "The primary components of TFOD2 include pre-trained models from the model zoo (such as SSD, Faster R-CNN, and EfficientDet), model architectures consisting of a backbone and detection head, data pipelines for handling images and annotations, training and evaluation tools, configuration files for setting model parameters, and inference tools for exporting models for deployment.\n",
        "\n",
        "Question 2: Differentiate between semantic segmentation and instance segmentation. Provide examples of where each might be used.\n",
        "\n",
        "->Semantic segmentation classifies each pixel in an image into a predefined category but does not distinguish between different objects of the same class. All objects belonging to the same class are treated as one region.\n",
        "Example: Road and lane detection in autonomous driving, where all road pixels are labeled as “road”.\n",
        "\n",
        "Instance segmentation not only classifies each pixel but also separates different instances of the same object class. Each object is detected individually with its own mask.\n",
        "Example: Counting people or vehicles in a crowded scene, where each person or car must be identified separately.\n",
        "\n",
        "\n",
        "Question 3: Explain the Mask R-CNN architecture. How does it extend Faster R-CNN?\n",
        "\n",
        "->Mask R-CNN is an advanced deep learning model used for instance segmentation. It is an extension of Faster R-CNN, designed to not only detect objects but also generate a pixel-level mask for each detected object.\n",
        "\n",
        "Mask R-CNN follows the Faster R-CNN pipeline with a backbone network (such as ResNet) for feature extraction and a Region Proposal Network (RPN) to generate object proposals. The proposed regions are then aligned using RoIAlign, which replaces RoIPool to preserve spatial accuracy.\n",
        "\n",
        "In addition to the classification and bounding box regression branches of Faster R-CNN, Mask R-CNN adds a parallel mask prediction branch that outputs a binary mask for each object. This enables precise instance-level segmentation.\n",
        "\n",
        "Question 4: Describe the purpose of masks in image segmentation. How are they used\n",
        "during training and inference?\n",
        "\n",
        "->In image segmentation, a mask is a pixel-level representation that indicates which pixels belong to a particular object or class. Masks help models understand the exact shape and boundaries of objects, rather than just their location.\n",
        "\n",
        "During training, ground-truth masks are used as target labels. The model learns to predict accurate pixel-wise regions by comparing predicted masks with true masks using loss functions such as mask loss.\n",
        "\n",
        "During inference, the trained model generates masks for new images. These predicted masks are overlaid on the image to highlight detected objects or regions, enabling tasks like object counting, medical image analysis, and scene understanding.\n",
        "\n",
        "\n",
        "Question 5: What are the steps involved in training a custom image segmentation\n",
        "model using TFOD2?\n",
        "\n",
        "->Dataset Preparation\n",
        "Collect images and create pixel-level annotations (masks). Convert the dataset into TFRecord format.\n",
        "\n",
        "Label Map Creation\n",
        "Define all object classes in a label_map.pbtxt file.\n",
        "\n",
        "Model Selection\n",
        "Choose a suitable pre-trained model (e.g., Mask R-CNN) from the TensorFlow Model Zoo.\n",
        "\n",
        "Configuration Setup\n",
        "Modify the pipeline .config file to set paths, number of classes, batch size, and learning rate.\n",
        "\n",
        "Training the Model\n",
        "Run the TFOD2 training script to fine-tune the model on the custom dataset.\n",
        "\n",
        "Evaluation\n",
        "Evaluate the model using validation data to measure performance (e.g., mAP, mask accuracy).\n",
        "\n",
        "Model Export & Inference\n",
        "Export the trained model for inference and test it on new images.\n",
        "\n"
      ],
      "metadata": {
        "id": "CgY0H8deU98Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "0XFhFcJcUoSN",
        "outputId": "752be012-135c-41a9-8545-c3da47bf5795"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!pip uninstall -y protobuf\\n!pip install protobuf==3.20.3\\n!rm -rf models\\n!git clone https://github.com/tensorflow/models.git\\n!apt-get install -y protobuf-compiler\\n!cd models/research && protoc object_detection/protos/*.proto --python_out=.\\n\\nimport sys\\nsys.path.append(\"models/research\")\\nsys.path.append(\"models/research/slim\")\\n\\nimport tensorflow as tf\\nfrom object_detection.utils import config_util\\n\\nprint(tf.__version__)\\n\\nconfig_dir = \"models/research/object_detection/configs/tf2\"\\nfor f in os.listdir(config_dir):\\n    print(f)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''Question 6: Write a Python script to install TFOD2 and verify its installation by printing\n",
        "the available model configs.\n",
        "'''\n",
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==3.20.3\n",
        "!rm -rf models\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "!apt-get install -y protobuf-compiler\n",
        "!cd models/research && protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"models/research\")\n",
        "sys.path.append(\"models/research/slim\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "config_dir = \"models/research/object_detection/configs/tf2\"\n",
        "for f in os.listdir(config_dir):\n",
        "    print(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Create a Python script to load a labeled dataset (in TFRecord format) and\n",
        "visualize the annotation masks over the images.\n",
        "'''\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def parse_tfrecord(example):\n",
        "    feature_description = {\n",
        "        \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"image/object/mask\": tf.io.VarLenFeature(tf.string),\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, feature_description)\n",
        "\n",
        "    image = tf.image.decode_jpeg(example[\"image/encoded\"], channels=3)\n",
        "    image = tf.cast(image, tf.uint8)\n",
        "\n",
        "    masks = tf.sparse.to_dense(example[\"image/object/mask\"], default_value=\"\")\n",
        "    masks = [tf.image.decode_png(m, channels=1) for m in masks]\n",
        "\n",
        "    return image, masks\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(\"sample_dataset.tfrecord\")\n",
        "dataset = dataset.map(parse_tfrecord)\n",
        "\n",
        "for image, masks in dataset.take(1):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    for mask in masks:\n",
        "        plt.imshow(mask, alpha=0.5)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "En5VYXDtZRDt",
        "outputId": "0be8ec92-8f88-4ab3-e166-2223bd109a30"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import tensorflow as tf\\nimport matplotlib.pyplot as plt\\n\\ndef parse_tfrecord(example):\\n    feature_description = {\\n        \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\\n        \"image/height\": tf.io.FixedLenFeature([], tf.int64),\\n        \"image/width\": tf.io.FixedLenFeature([], tf.int64),\\n        \"image/object/mask\": tf.io.VarLenFeature(tf.string),\\n    }\\n\\n    example = tf.io.parse_single_example(example, feature_description)\\n\\n    image = tf.image.decode_jpeg(example[\"image/encoded\"], channels=3)\\n    image = tf.cast(image, tf.uint8)\\n\\n    masks = tf.sparse.to_dense(example[\"image/object/mask\"], default_value=\"\")\\n    masks = [tf.image.decode_png(m, channels=1) for m in masks]\\n\\n    return image, masks\\n\\ndataset = tf.data.TFRecordDataset(\"sample_dataset.tfrecord\")\\ndataset = dataset.map(parse_tfrecord)\\n\\nfor image, masks in dataset.take(1):\\n    plt.figure(figsize=(6, 6))\\n    plt.imshow(image)\\n    for mask in masks:\\n        plt.imshow(mask, alpha=0.5)\\n    plt.axis(\"off\")\\n    plt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Using a pre-trained Mask R-CNN model, write a code snippet to perform\n",
        "inference on a single image and plot the predicted masks.\n",
        "'''\n",
        "\n",
        "!pip install tensorflow==2.19.0\n",
        "!pip install tf-slim\n",
        "!pip install tensorflow-models-official==2.19.1\n",
        "!pip install opencv-python matplotlib\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "MODEL_DATE = \"20200711\"\n",
        "MODEL_NAME = \"mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu_tpu-8\"\n",
        "MODEL_TAR_FILENAME = MODEL_NAME + \".tar.gz\"\n",
        "DOWNLOAD_BASE = \"http://download.tensorflow.org/models/object_detection/tf2/\" + MODEL_DATE + \"/\"\n",
        "PATH_TO_CKPT = MODEL_NAME + \"/saved_model\"\n",
        "\n",
        "if not os.path.exists(MODEL_NAME):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_TAR_FILENAME, MODEL_TAR_FILENAME)\n",
        "    tar_file = tarfile.open(MODEL_TAR_FILENAME)\n",
        "    tar_file.extractall()\n",
        "    tar_file.close()\n",
        "\n",
        "detect_fn = tf.saved_model.load(PATH_TO_CKPT)\n",
        "\n",
        "IMAGE_URL = \"https://tensorflow.org/images/surf.jpg\"\n",
        "IMAGE_PATH = \"test_image.jpg\"\n",
        "urllib.request.urlretrieve(IMAGE_URL, IMAGE_PATH)\n",
        "image_np = cv2.cvtColor(cv2.imread(IMAGE_PATH), cv2.COLOR_BGR2RGB)\n",
        "input_tensor = tf.convert_to_tensor([image_np])\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "num_detections = int(detections.pop('num_detections'))\n",
        "detections = {key: value[0, :num_detections].numpy()\n",
        "              for key, value in detections.items()}\n",
        "detections['num_detections'] = num_detections\n",
        "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "PATH_TO_LABELS = tf.keras.utils.get_file(\n",
        "    'mscoco_label_map.pbtxt',\n",
        "    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        ")\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np,\n",
        "    detections['detection_boxes'],\n",
        "    detections['detection_classes'],\n",
        "    detections['detection_scores'],\n",
        "    category_index,\n",
        "    instance_masks=detections.get('detection_masks'),\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=2,\n",
        "    min_score_thresh=0.5\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(image_np)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "WWv390TjZyIU",
        "outputId": "f1316af2-cdac-4ecb-9516-e926056fe383"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!pip install tensorflow==2.19.0\\n!pip install tf-slim\\n!pip install tensorflow-models-official==2.19.1\\n!pip install opencv-python matplotlib\\n\\nimport tensorflow as tf\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport cv2\\nimport urllib.request\\nimport tarfile\\nimport os\\nfrom object_detection.utils import label_map_util\\nfrom object_detection.utils import visualization_utils as viz_utils\\n\\nMODEL_DATE = \"20200711\"\\nMODEL_NAME = \"mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu_tpu-8\"\\nMODEL_TAR_FILENAME = MODEL_NAME + \".tar.gz\"\\nDOWNLOAD_BASE = \"http://download.tensorflow.org/models/object_detection/tf2/\" + MODEL_DATE + \"/\"\\nPATH_TO_CKPT = MODEL_NAME + \"/saved_model\"\\n\\nif not os.path.exists(MODEL_NAME):\\n    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_TAR_FILENAME, MODEL_TAR_FILENAME)\\n    tar_file = tarfile.open(MODEL_TAR_FILENAME)\\n    tar_file.extractall()\\n    tar_file.close()\\n\\ndetect_fn = tf.saved_model.load(PATH_TO_CKPT)\\n\\nIMAGE_URL = \"https://tensorflow.org/images/surf.jpg\"\\nIMAGE_PATH = \"test_image.jpg\"\\nurllib.request.urlretrieve(IMAGE_URL, IMAGE_PATH)\\nimage_np = cv2.cvtColor(cv2.imread(IMAGE_PATH), cv2.COLOR_BGR2RGB)\\ninput_tensor = tf.convert_to_tensor([image_np])\\ndetections = detect_fn(input_tensor)\\n\\nnum_detections = int(detections.pop(\\'num_detections\\'))\\ndetections = {key: value[0, :num_detections].numpy()\\n              for key, value in detections.items()}\\ndetections[\\'num_detections\\'] = num_detections\\ndetections[\\'detection_classes\\'] = detections[\\'detection_classes\\'].astype(np.int64)\\n\\nPATH_TO_LABELS = tf.keras.utils.get_file(\\n    \\'mscoco_label_map.pbtxt\\',\\n    \\'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/mscoco_label_map.pbtxt\\'\\n)\\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\\n\\nviz_utils.visualize_boxes_and_labels_on_image_array(\\n    image_np,\\n    detections[\\'detection_boxes\\'],\\n    detections[\\'detection_classes\\'],\\n    detections[\\'detection_scores\\'],\\n    category_index,\\n    instance_masks=detections.get(\\'detection_masks\\'),\\n    use_normalized_coordinates=True,\\n    line_thickness=2,\\n    min_score_thresh=0.5\\n)\\n\\nplt.figure(figsize=(12, 8))\\nplt.imshow(image_np)\\nplt.axis(\\'off\\')\\nplt.show()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python script to evaluate a trained TFOD2 Mask R-CNN model and\n",
        "plot the Precision-Recall curve.'''\n",
        "\n",
        "!pip install tensorflow==2.19.0\n",
        "!pip install tf-slim\n",
        "!pip install tensorflow-models-official==2.19.1\n",
        "!pip install matplotlib\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.metrics import coco_evaluation\n",
        "from object_detection.metrics import object_detection_evaluation\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "PATH_TO_MODEL_DIR = \"trained_mask_rcnn_model\"\n",
        "PATH_TO_CFG = os.path.join(PATH_TO_MODEL_DIR, \"pipeline.config\")\n",
        "PATH_TO_LABELS = \"label_map.pbtxt\"\n",
        "PATH_TO_TEST_TFRECORD = \"test.record\"\n",
        "\n",
        "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
        "model_config = configs['model']\n",
        "detection_model = tf.saved_model.load(os.path.join(PATH_TO_MODEL_DIR, \"saved_model\"))\n",
        "\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS)\n",
        "\n",
        "def load_dataset(tfrecord_path):\n",
        "    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "    return raw_dataset\n",
        "\n",
        "test_dataset = load_dataset(PATH_TO_TEST_TFRECORD)\n",
        "\n",
        "def run_inference(image_tensor):\n",
        "    input_tensor = tf.convert_to_tensor([image_tensor])\n",
        "    detections = detection_model(input_tensor)\n",
        "    return detections\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "all_labels = []\n",
        "all_scores = []\n",
        "\n",
        "for record in tf.data.TFRecordDataset(PATH_TO_TEST_TFRECORD):\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(record.numpy())\n",
        "    height = int(example.features.feature['image/height'].int64_list.value[0])\n",
        "    width = int(example.features.feature['image/width'].int64_list.value[0])\n",
        "    image_raw = example.features.feature['image/encoded'].bytes_list.value[0]\n",
        "    image_np = tf.image.decode_jpeg(image_raw, channels=3).numpy()\n",
        "\n",
        "    detections = run_inference(image_np)\n",
        "    scores = detections['detection_scores'][0].numpy()\n",
        "    classes = detections['detection_classes'][0].numpy()\n",
        "\n",
        "    all_scores.extend(scores)\n",
        "    all_labels.extend(classes)\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_scores = np.array(all_scores)\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(all_labels, all_scores, pos_label=1)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "I4Czx6sEaBvl",
        "outputId": "9163f92e-4109-49f9-efd6-1216d54c5309"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!pip install tensorflow==2.19.0\\n!pip install tf-slim\\n!pip install tensorflow-models-official==2.19.1\\n!pip install matplotlib\\n\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\nimport os\\nfrom object_detection.utils import config_util\\nfrom object_detection.metrics import coco_evaluation\\nfrom object_detection.metrics import object_detection_evaluation\\nfrom object_detection.utils import label_map_util\\n\\nPATH_TO_MODEL_DIR = \"trained_mask_rcnn_model\"\\nPATH_TO_CFG = os.path.join(PATH_TO_MODEL_DIR, \"pipeline.config\")\\nPATH_TO_LABELS = \"label_map.pbtxt\"\\nPATH_TO_TEST_TFRECORD = \"test.record\"\\n\\nconfigs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\\nmodel_config = configs[\\'model\\']\\ndetection_model = tf.saved_model.load(os.path.join(PATH_TO_MODEL_DIR, \"saved_model\"))\\n\\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS)\\n\\ndef load_dataset(tfrecord_path):\\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\\n    return raw_dataset\\n\\ntest_dataset = load_dataset(PATH_TO_TEST_TFRECORD)\\n\\ndef run_inference(image_tensor):\\n    input_tensor = tf.convert_to_tensor([image_tensor])\\n    detections = detection_model(input_tensor)\\n    return detections\\n\\nfrom sklearn.metrics import precision_recall_curve, auc\\nimport numpy as np\\n\\nall_labels = []\\nall_scores = []\\n\\nfor record in tf.data.TFRecordDataset(PATH_TO_TEST_TFRECORD):\\n    example = tf.train.Example()\\n    example.ParseFromString(record.numpy())\\n    height = int(example.features.feature[\\'image/height\\'].int64_list.value[0])\\n    width = int(example.features.feature[\\'image/width\\'].int64_list.value[0])\\n    image_raw = example.features.feature[\\'image/encoded\\'].bytes_list.value[0]\\n    image_np = tf.image.decode_jpeg(image_raw, channels=3).numpy()\\n\\n    detections = run_inference(image_np)\\n    scores = detections[\\'detection_scores\\'][0].numpy()\\n    classes = detections[\\'detection_classes\\'][0].numpy()\\n\\n    all_scores.extend(scores)\\n    all_labels.extend(classes)\\n\\nall_labels = np.array(all_labels)\\nall_scores = np.array(all_scores)\\n\\nprecision, recall, thresholds = precision_recall_curve(all_labels, all_scores, pos_label=1)\\n\\nplt.figure(figsize=(8,6))\\nplt.plot(recall, precision, marker=\\'.\\')\\nplt.xlabel(\\'Recall\\')\\nplt.ylabel(\\'Precision\\')\\nplt.title(\\'Precision-Recall Curve\\')\\nplt.grid(True)\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working with a city surveillance team to identify illegal parking\n",
        "zones from street camera images. The model you built detects cars using bounding\n",
        "boxes, but the team reports inaccurate overlaps with sidewalks and fails in complex\n",
        "street scenes.\n",
        "How would you refine your model to improve accuracy, especially around object\n",
        "boundaries? What segmentation strategy and tools would you use?\n",
        "\n",
        "\n",
        "->To improve the accuracy of detecting cars in complex street scenes and reduce overlaps with sidewalks:\n",
        "\n",
        "Switch to Instance Segmentation\n",
        "\n",
        "Instead of only bounding boxes (object detection), use Mask R-CNN or similar instance segmentation models.\n",
        "\n",
        "Instance segmentation provides pixel-level masks for each object, accurately separating cars from sidewalks and other objects.\n",
        "\n",
        "Refine Data and Annotations\n",
        "\n",
        "Include boundary-sensitive annotations (masks that precisely cover cars).\n",
        "\n",
        "Augment the dataset with crowded scenes, occlusions, and varied angles.\n",
        "\n",
        "Segmentation Strategies\n",
        "\n",
        "Use Mask R-CNN for object-level masks.\n",
        "\n",
        "Alternatively, for very dense scenes, consider semantic + instance segmentation hybrid (e.g., Panoptic Segmentation).\n",
        "\n",
        "Tools and Libraries\n",
        "\n",
        "TensorFlow Object Detection API (TFOD2) with Mask R-CNN.\n",
        "\n",
        "Detectron2 (from Facebook AI) for advanced instance segmentation.\n",
        "\n",
        "OpenCV / PIL for preprocessing and visualization.\n",
        "\n",
        "Post-processing\n",
        "\n",
        "Apply Non-Max Suppression and mask refinement (e.g., Conditional Random Fields) to improve object boundaries.\n",
        "\n",
        "Summary: Replace plain object detection with Mask R-CNN or similar instance segmentation, refine your dataset, and use boundary-aware post-processing to improve detection accuracy around sidewalks and complex street layouts."
      ],
      "metadata": {
        "id": "hlzlfnUAmxDn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZ6_SORomd2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}